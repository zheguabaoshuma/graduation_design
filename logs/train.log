25-04-17 02:23:00.429 - INFO:   name: Diff-IF-mif-train
  phase: train
  gpu_ids: [0]
  path:[
    log: logs
    tb_logger: tb_logger
    results: results
    checkpoint: checkpoint
    resume_state: None
  ]
  datasets:[
    train:[
      dataset: Harvard
      name: train
      dataroot: dataset/My_dataset/train
      batch_size: 2
      num_workers: 2
      use_shuffle: True
      data_len: -1
    ]
    val:[
      dataset: Test_mif
      name: val
      dataroot: dataset/My_dataset/eval
      data_len: -1
    ]
  ]
  model:[
    which_model_G: diffif
    finetune_norm: False
    unet_denoising:[
      in_channel: 3
      out_channel: 1
      inner_channel: 48
      channel_multiplier: [1, 2, 4, 6]
      attn_res: [16]
      res_blocks: 2
      dropout: 0.1
    ]
    unet_refine:[
      in_channel: 1
      out_channel: 1
    ]
    beta_schedule:[
      train:[
        schedule: linear
        n_timestep: 1000
        linear_start: 1e-06
        linear_end: 0.01
      ]
      val:[
        schedule: linear
        n_timestep: 1000
        linear_start: 1e-06
        linear_end: 0.01
      ]
    ]
    diffusion:[
      channels: 3
      conditional: True
    ]
  ]
  train:[
    n_iter: 16000
    val_freq: 2000.0
    save_checkpoint_freq: 2000.0
    print_freq: 100
    optimizer:[
      type: adam
      lr: 0.0001
    ]
  ]

25-04-17 02:23:00.432 - INFO: Dataset [Harvard_Dataset - train] is created.
25-04-17 02:23:00.432 - INFO: Dataset [Harvard_Test_Dataset - val] is created.
25-04-17 02:23:00.432 - INFO: Initial Dataset Finished
25-04-17 02:23:01.722 - INFO: Initialization method [orthogonal]
25-04-17 02:23:07.969 - INFO: Network G structure: GaussianDiffusion, with parameters: 266,595,276
25-04-17 02:23:07.969 - INFO: GaussianDiffusion(
  (denoise_fn): UNet(
    (noise_level_mlp): Sequential(
      (0): PositionalEncoding()
      (1): Linear(in_features=48, out_features=192, bias=True)
      (2): Swish()
      (3): Linear(in_features=192, out_features=48, bias=True)
    )
    (downs): ModuleList(
      (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1-2): 2 x ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=48, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 48, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 48, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Identity()
        )
      )
      (3): Downsample(
        (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
      (4): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=96, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 48, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 96, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (5): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=96, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 96, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 96, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Identity()
        )
      )
      (6): Downsample(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
      (7): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=192, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 96, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 192, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (8): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=192, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 192, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 192, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Identity()
        )
      )
      (9): Downsample(
        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
      (10): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=288, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 192, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(192, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 288, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Conv2d(192, 288, kernel_size=(1, 1), stride=(1, 1))
        )
        (attn): SelfAttention(
          (norm): GroupNorm(24, 288, eps=1e-05, affine=True)
          (qkv): Conv2d(288, 864, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (out): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (11): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=288, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 288, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 288, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Identity()
        )
        (attn): SelfAttention(
          (norm): GroupNorm(24, 288, eps=1e-05, affine=True)
          (qkv): Conv2d(288, 864, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (out): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
    (mid): ModuleList(
      (0): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=288, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 288, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 288, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Identity()
        )
        (attn): SelfAttention(
          (norm): GroupNorm(24, 288, eps=1e-05, affine=True)
          (qkv): Conv2d(288, 864, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (out): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (1): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=288, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 288, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 288, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Identity()
        )
      )
    )
    (ups): ModuleList(
      (0-1): 2 x ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=288, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 576, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(576, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 288, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1))
        )
        (attn): SelfAttention(
          (norm): GroupNorm(24, 288, eps=1e-05, affine=True)
          (qkv): Conv2d(288, 864, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (out): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (2): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=288, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 480, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(480, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 288, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Conv2d(480, 288, kernel_size=(1, 1), stride=(1, 1))
        )
        (attn): SelfAttention(
          (norm): GroupNorm(24, 288, eps=1e-05, affine=True)
          (qkv): Conv2d(288, 864, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (out): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (3): Upsample(
        (up): Upsample(scale_factor=2.0, mode='nearest')
        (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (4): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=192, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 480, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(480, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 192, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (5): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=192, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 384, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 192, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (6): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=192, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 288, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(288, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 192, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (7): Upsample(
        (up): Upsample(scale_factor=2.0, mode='nearest')
        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (8): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=96, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 288, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(288, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 96, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (9): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=96, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 192, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 96, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (10): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=96, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 144, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(144, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 96, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Conv2d(144, 96, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (11): Upsample(
        (up): Upsample(scale_factor=2.0, mode='nearest')
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (12): ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=48, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 144, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(144, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 48, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (13-14): 2 x ResnetBlocWithAttn(
        (res_block): ResnetBlock(
          (noise_func): FeatureWiseAffine(
            (noise_func): Sequential(
              (0): Linear(in_features=48, out_features=48, bias=True)
            )
          )
          (block1): Block(
            (block): Sequential(
              (0): GroupNorm(24, 96, eps=1e-05, affine=True)
              (1): Swish()
              (2): Identity()
              (3): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (block2): Block(
            (block): Sequential(
              (0): GroupNorm(24, 48, eps=1e-05, affine=True)
              (1): Swish()
              (2): Dropout(p=0.1, inplace=False)
              (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (res_conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
    (final_conv): Block(
      (block): Sequential(
        (0): GroupNorm(24, 48, eps=1e-05, affine=True)
        (1): Swish()
        (2): Identity()
        (3): Conv2d(48, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (refinement_fn): WaveUIR_fn(
    (noise_level_mlp): Sequential(
      (0): PositionalEncoding()
      (1): Linear(in_features=48, out_features=192, bias=True)
      (2): Swish()
      (3): Linear(in_features=192, out_features=48, bias=True)
    )
    (patch_embed): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (patch_embed_cond): OverlapPatchEmbed(
      (proj): Conv2d(2, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (reduce_channel_level1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1))
    (reduce_channel_level2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
    (reduce_channel_level3): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
    (reduce_channel_level4): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
    (reduce_high_bands_channel_level1): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
    (reduce_high_bands_channel_level2): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
    (reduce_high_bands_channel_level3): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1))
    (encoder_level1): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(48, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(127, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(48, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(127, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(48, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(127, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(48, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(127, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (DWTBlock1): DWTBlock(
      (dwt): DWT()
      (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (encoder_level2): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (4): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (5): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (DWTBlock2): DWTBlock(
      (dwt): DWT()
      (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (encoder_level3): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (4): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (5): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (DWTBlock3): DWTBlock(
      (dwt): DWT()
      (conv): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (latent): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (4): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (5): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (6): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (7): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (encoder_level1_cond): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(48, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(127, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(48, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(127, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(48, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(127, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (project_out): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(48, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(127, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(127, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (DWTBlock1_cond): DWTBlock(
      (dwt): DWT()
      (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (encoder_level2_cond): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (4): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (5): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (DWTBlock2_cond): DWTBlock(
      (dwt): DWT()
      (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (encoder_level3_cond): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (4): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (5): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (DWTBlock3_cond): DWTBlock(
      (dwt): DWT()
      (conv): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (latent_cond): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (4): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (5): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (6): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (7): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (project_out): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(384, 1021, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(1021, 1021, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(1021, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (Degradation_weights): PromptWeightsClassify(
      (conv1): Conv2d(1152, 288, kernel_size=(1, 1), stride=(1, 1))
      (conv2): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1))
      (linear_layer): Linear(in_features=64, out_features=5, bias=True)
    )
    (WaveCoeffFilter3): WaveCoeffFilter()
    (WaveCoeffFilter2): WaveCoeffFilter()
    (WaveCoeffFilter1): WaveCoeffFilter()
    (IWTBlock3): IWTBlock(
      (iwt): IWT()
      (conv_l_iwt): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (conv_h_iwt): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (channel_attention): ChannelAttention(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (max_pool): AdaptiveMaxPool2d(output_size=1)
        (fc1): Conv2d(1536, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (relu1): ReLU()
        (fc2): Conv2d(96, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sigmoid): Sigmoid()
      )
      (out_conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (reduce_chan_level3): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (decoder_level3): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (4): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (5): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (project_out): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(192, 510, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(510, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(510, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (IWTBlock2): IWTBlock(
      (iwt): IWT()
      (conv_l_iwt): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (conv_h_iwt): Conv2d(576, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (channel_attention): ChannelAttention(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (max_pool): AdaptiveMaxPool2d(output_size=1)
        (fc1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (relu1): ReLU()
        (fc2): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sigmoid): Sigmoid()
      )
      (out_conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (reduce_chan_level2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (decoder_level2): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (4): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (5): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (IWTBlock1): IWTBlock(
      (iwt): IWT()
      (conv_l_iwt): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (conv_h_iwt): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (channel_attention): ChannelAttention(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (max_pool): AdaptiveMaxPool2d(output_size=1)
        (fc1): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (relu1): ReLU()
        (fc2): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (sigmoid): Sigmoid()
      )
      (out_conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (decoder_level1): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (refinement): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (project_out): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(96, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (project_out): Conv2d(255, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (output): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (loss_func): L1Loss()
  (grad_loss): Grad_loss()
  (ssim_loss): SSIM_loss()
)
25-04-17 02:23:07.974 - INFO: Initial Model Finished
